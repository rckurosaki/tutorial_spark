# 5. Conclusão

Ao longo da produção da documentação, do tutorial e até mesmo a partir das experiências profissionais dos integrantes do grupo, observamos as capacidades que o Apache Spark™ possui.
Até então estávamos acostumados a manipular pequenos conjunto de dados e quando tínhamos um volume maior, todo desempenho que precisávamos ganhar para fazer a manipulação eficiente desses dados se baseava na otimização de buscas, criação de índices, implementação de dispositivos que facilitava operações CRUD.
A pesquisa para esse trabalho nos proporcionou adquirir conhecimento não só da ferramenta como os conceitos que são aplicados nelas que habilitam toda a velocidade e eficiência para trabalhar com grandes volumes de dados.
A intenção do documento e tutorial aqui apresentado foi compartilhar esse conhecimento de uma maneira centralizada com objetivo de permitir a qualquer um sem conhecimento prévio de Big Data, 
entender os conceitos básicos e ganhar experiência utilizando a plataforma Apache Spark™.


Não tivemos nenhuma dificuldade impeditiva durante a produção. Infelizmente não conseguimos demonstrar todas as capacidades de processamento distribuído e paralelizado do Spark que são seus pontos fortes, 
mas acreditamos na utilidade do tutorial, compartilhamento de conhecimento e induzimos curiosidade através dos textos das possibilidades da ferramenta.


[Voltar para Sumário](/tutorial_spark#sumário)  
[Próxima Seção - Referências](/seções/referências.md)